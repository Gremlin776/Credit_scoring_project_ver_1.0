#!/usr/bin/env python3
"""
EDA (Exploratory Data Analysis) –¥–ª—è Credit Scoring –ø—Ä–æ–µ–∫—Ç–∞
–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import sys
import io
import warnings
warnings.filterwarnings('ignore')

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='ignore')

def calculate_imbalance_ratio(target_series):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ –∫–ª–∞—Å—Å–æ–≤"""
    class_counts = target_series.value_counts()
    majority_class = class_counts.max()
    minority_class = class_counts.min()
    imbalance_ratio = minority_class / majority_class
    return imbalance_ratio, class_counts

def analyze_correlations(df, target_column, top_n=10):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    correlation_with_target = df[numeric_columns].corr()[target_column].abs().sort_values(ascending=False)
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ —Ç–æ–ø –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    top_correlations = correlation_with_target[correlation_with_target.index != target_column].head(top_n)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–æ—Ä–æ–≥ > 0.1)
    high_corr_features = top_correlations[top_correlations > 0.1]
    medium_corr_features = top_correlations[(top_correlations > 0.05) & (top_correlations <= 0.1)]
    
    return top_correlations, high_corr_features, medium_corr_features

def detect_data_issues(df):
    """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –≤ –¥–∞–Ω–Ω—ã—Ö"""
    issues = []
    
    # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    missing_data = df.isnull().sum()
    if missing_data.sum() > 0:
        issues.append(f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {missing_data[missing_data > 0].to_dict()}")
    
    # –í—ã–±—Ä–æ—Å—ã –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    outlier_info = {}
    
    for col in numeric_columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
        outlier_percent = len(outliers) / len(df) * 100
        
        if outlier_percent > 5:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 5% - –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã
            outlier_info[col] = f"{outlier_percent:.1f}% –≤—ã–±—Ä–æ—Å–æ–≤"
    
    if outlier_info:
        issues.append(f"–ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã: {outlier_info}")
    
    # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
    categorical_checks = {
        'SEX': [1, 2],
        'EDUCATION': [1, 2, 3, 4],
        'MARRIAGE': [1, 2, 3]
    }
    
    for col, allowed_values in categorical_checks.items():
        if col in df.columns:
            unexpected_values = set(df[col].unique()) - set(allowed_values)
            if unexpected_values:
                issues.append(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ {col}: {unexpected_values}")
    
    return issues

def analyze_feature_distributions(df, target_column):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    feature_analysis = {}
    
    for col in numeric_columns:
        if col != target_column:
            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç –Ω–∞ —Ä–∞–∑–ª–∏—á–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π
            group_0 = df[df[target_column] == 0][col].dropna()
            group_1 = df[df[target_column] == 1][col].dropna()
            
            if len(group_0) > 0 and len(group_1) > 0:
                # T-—Ç–µ—Å—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π, U-—Ç–µ—Å—Ç –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                if col in ['LIMIT_BAL', 'AGE']:  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —ç—Ç–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    stat, p_value = stats.ttest_ind(group_0, group_1, nan_policy='omit')
                else:
                    stat, p_value = stats.mannwhitneyu(group_0, group_1, nan_policy='omit')
                
                feature_analysis[col] = {
                    'p_value': p_value,
                    'mean_0': group_0.mean(),
                    'mean_1': group_1.mean(),
                    'significant': p_value < 0.05
                }
    
    return feature_analysis

def generate_insights(eda_results):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
    insights = []
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
    imbalance_ratio = eda_results['imbalance_ratio']
    if imbalance_ratio < 0.3:
        insights.append("üî¥ –í–´–°–û–ö–ò–ô –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å class_weight='balanced' –∏–ª–∏ oversampling")
    elif imbalance_ratio < 0.5:
        insights.append("üü° –°–†–ï–î–ù–ò–ô –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã")
    else:
        insights.append("üü¢ –ù–ò–ó–ö–ò–ô –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í - –¥–∞–Ω–Ω—ã–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω—ã")
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    high_corr_count = len(eda_results['high_corr_features'])
    medium_corr_count = len(eda_results['medium_corr_features'])
    
    if high_corr_count > 0:
        insights.append(f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {high_corr_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –í–´–°–û–ö–û–ô –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")
    if medium_corr_count > 0:
        insights.append(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {medium_corr_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ –°–†–ï–î–ù–ï–ô –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π")
    
    if high_corr_count == 0 and medium_corr_count == 0:
        insights.append("‚ö†Ô∏è –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π - –≤–æ–∑–º–æ–∂–Ω–æ, —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–µ feature engineering")
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –¥–∞–Ω–Ω—ã—Ö
    if len(eda_results['data_issues']) > 0:
        insights.append("üö® –û–ë–ù–ê–†–£–ñ–ï–ù–´ –ü–†–û–ë–õ–ï–ú–´ –í –î–ê–ù–ù–´–• - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞")
    else:
        insights.append("‚úÖ –î–∞–Ω–Ω—ã–µ –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏")
    
    # –ê–Ω–∞–ª–∏–∑ –∑–Ω–∞—á–∏–º—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    significant_features = [feature for feature, info in eda_results['feature_analysis'].items() 
                          if info['significant']]
    insights.append(f"üìà {len(significant_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–º–µ—é—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏")
    
    return insights

def run_eda():
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ EDA –∞–Ω–∞–ª–∏–∑–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –≤—ã–≤–æ–¥–∞–º–∏"""
    print("üéØ –ó–ê–ü–£–°–ö –ê–î–ê–ü–¢–ò–í–ù–û–ì–û EDA –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
    os.makedirs('reports', exist_ok=True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    plt.style.use('seaborn-v0_8')
    sns.set_palette("husl")
    pd.set_option('display.max_columns', None)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    try:
        df = pd.read_csv('data/raw/UCI_Credit_Card.csv')
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
    target_column = 'default.payment.next.month'
    if target_column not in df.columns:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ –¥—Ä—É–≥–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º
        possible_targets = ['default', 'target', 'y', 'Default']
        for col in possible_targets:
            if col in df.columns:
                target_column = col
                break
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é")
            return
    
    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"\nüìä –ë–ê–ó–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
    print(f"–†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
    print(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: '{target_column}'")
    print(f"–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\n{df.dtypes.value_counts()}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    print(f"\nüéØ –ê–ù–ê–õ–ò–ó –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô:")
    target_counts = df[target_column].value_counts()
    target_percent = df[target_column].value_counts(normalize=True) * 100
    
    for val, count in target_counts.items():
        print(f"–ö–ª–∞—Å—Å {val}: {count} –∑–∞–ø–∏—Å–µ–π ({target_percent[val]:.2f}%)")
    
    # –†–∞—Å—á–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
    imbalance_ratio, class_counts = calculate_imbalance_ratio(df[target_column])
    print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞: {imbalance_ratio:.3f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    sns.countplot(data=df, x=target_column)
    plt.title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n(–¥–∏—Å–±–∞–ª–∞–Ω—Å: {imbalance_ratio:.3f})')
    plt.xlabel('–ö–ª–∞—Å—Å')
    plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
    
    plt.subplot(1, 2, 2)
    plt.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%', 
            colors=['lightblue', 'lightcoral'])
    plt.title('–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
    
    plt.tight_layout()
    plt.savefig('reports/target_distribution.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    print(f"\nüîó –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó:")
    top_correlations, high_corr_features, medium_corr_features = analyze_correlations(df, target_column)
    
    print("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    for feature, corr in top_correlations.head(10).items():
        significance = "üî¥ –í–´–°–û–ö–ê–Ø" if corr > 0.1 else "üü° –°–†–ï–î–ù–Ø–Ø" if corr > 0.05 else "‚ö™ –ù–ò–ó–ö–ê–Ø"
        print(f"  {significance} {feature}: {corr:.4f}")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    if len(numeric_columns) > 1:
        plt.figure(figsize=(12, 8))
        corr_matrix = df[numeric_columns].corr()
        
        # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫—É –¥–ª—è –≤–µ—Ä—Ö–Ω–µ–≥–æ —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫–∞
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        
        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                    square=True, fmt='.2f', cbar_kws={"shrink": .8})
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=16)
        plt.tight_layout()
        plt.savefig('reports/correlation_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\nüìà –ê–ù–ê–õ–ò–ó –ß–ò–°–õ–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
    numeric_columns = df.select_dtypes(include=[np.number]).columns
    numeric_data = df[numeric_columns].describe()
    print(numeric_data)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    key_features = list(high_corr_features.index[:6]) if len(high_corr_features) > 0 else numeric_columns[:6]
    
    if len(key_features) > 0:
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.ravel()
        
        for i, col in enumerate(key_features[:6]):
            if i < len(axes):
                df[col].hist(bins=50, alpha=0.7, color='skyblue', edgecolor='black', ax=axes[i])
                axes[i].set_title(f'{col}\n(–∫–æ—Ä—Ä: {top_correlations.get(col, 0):.3f})')
                axes[i].set_xlabel(col)
        
        plt.tight_layout()
        plt.savefig('reports/key_features_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
    categorical_columns = ['SEX', 'EDUCATION', 'MARRIAGE']
    available_categorical = [col for col in categorical_columns if col in df.columns]
    
    for col in available_categorical:
        if col in df.columns:
            value_counts = df[col].value_counts().sort_index()
            print(f"\n{col}:")
            for val, count in value_counts.items():
                print(f"  {val}: {count} ({count/len(df)*100:.1f}%)")
    
    # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –≤ –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüîç –î–ï–¢–ï–ö–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ë–õ–ï–ú –î–ê–ù–ù–´–•:")
    data_issues = detect_data_issues(df)
    if len(data_issues) > 0:
        for issue in data_issues:
            print(f"  ‚ö†Ô∏è {issue}")
    else:
        print("  ‚úÖ –°–µ—Ä—å–µ–∑–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –ü–†–ò–ó–ù–ê–ö–û–í:")
    feature_analysis = analyze_feature_distributions(df, target_column)
    significant_features = [feature for feature, info in feature_analysis.items() if info['significant']]
    
    print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ –∑–Ω–∞—á–∏–º—ã–º–∏ —Ä–∞–∑–ª–∏—á–∏—è–º–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏: {len(significant_features)}")
    if len(significant_features) > 0:
        print("–°–∞–º—ã–µ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:")
        for feature in significant_features[:5]:
            info = feature_analysis[feature]
            print(f"  {feature}: p-value={info['p_value']:.6f}")
    
    # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤
    eda_results = {
        'dataset_shape': df.shape,
        'target_distribution': target_counts.to_dict(),
        'imbalance_ratio': imbalance_ratio,
        'top_correlations': top_correlations.to_dict(),
        'high_corr_features': high_corr_features.to_dict(),
        'medium_corr_features': medium_corr_features.to_dict(),
        'data_issues': data_issues,
        'feature_analysis': feature_analysis,
        'numeric_summary': numeric_data.to_dict()
    }
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã–≤–æ–¥–æ–≤
    print(f"\nüéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –í–´–í–û–î–´ –ù–ê –û–°–ù–û–í–ï –î–ê–ù–ù–´–•:")
    print("=" * 50)
    
    insights = generate_insights(eda_results)
    for i, insight in enumerate(insights, 1):
        print(f"{i}. {insight}")
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø:")
    if imbalance_ratio < 0.3:
        print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ class_weight='balanced' –≤ –º–æ–¥–µ–ª—è—Ö")
        print("  ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ SMOTE –∏–ª–∏ –¥—Ä—É–≥–∏–µ oversampling –º–µ—Ç–æ–¥—ã")
        print("  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç—Ä–∏–∫–∏: ROC-AUC, Precision-Recall")
    
    if len(high_corr_features) > 0:
        print("  ‚Ä¢ –í–∫–ª—é—á–∏—Ç–µ –≤—ã—Å–æ–∫–æ–∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ –º–æ–¥–µ–ª—å")
        print("  ‚Ä¢ –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∫–ª—é—á–µ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
    
    if len(data_issues) > 0:
        print("  ‚Ä¢ –í—ã–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º")
        print("  ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –≤—ã–±—Ä–æ—Å—ã –∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ EDA
    import json
    os.makedirs('reports', exist_ok=True)
    with open('reports/eda_report.json', 'w', encoding='utf-8') as f:
        json.dump(eda_results, f, indent=2, default=str)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—ã–≤–æ–¥–æ–≤
    with open('reports/eda_insights.txt', 'w', encoding='utf-8') as f:
        f.write("–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –í–´–í–û–î–´ EDA\n")
        f.write("=" * 40 + "\n\n")
        for insight in insights:
            f.write(f"‚Ä¢ {insight}\n")
    
    print(f"\n‚úÖ EDA –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print(f"üìÅ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ reports/")
    print(f"   - eda_report.json (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)")
    print(f"   - eda_insights.txt (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã)")
    print(f"   - *.png (–≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)")
    
    return eda_results

if __name__ == "__main__":
    import os
    run_eda()
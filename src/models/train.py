import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, roc_curve, classification_report
import matplotlib.pyplot as plt
import mlflow
import mlflow.sklearn
from .pipeline import create_model_pipeline, get_feature_names
import json
import os
import seaborn as sns
import sys
import io

# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='ignore')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='ignore')

def load_data_with_features():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏—á–∞–º–∏"""
    try:
        df = pd.read_csv('data/processed/data_with_features.csv')
        
        # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
        y = df['default']
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ - –∏—Å–∫–ª—é—á–∞–µ–º —Ü–µ–ª–µ–≤—É—é –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        exclude_columns = ['default', 'AGE_GROUP']
        X = df.drop(columns=exclude_columns, errors='ignore')
        
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {X.shape}")
        return X, y
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None, None

def hyperparameter_tuning(pipeline, model_type, X_train, y_train):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é GridSearchCV"""
    print(f"üîç GridSearchCV –¥–ª—è {model_type}...")
    
    param_grids = {
        'logistic': {
            'classifier__C': [0.1, 1.0, 10.0],
            'classifier__class_weight': ['balanced']
        },
        'random_forest': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [10, 15],
            'classifier__class_weight': ['balanced']
        },
        'gradient_boosting': {
            'classifier__n_estimators': [100, 200],
            'classifier__learning_rate': [0.1, 0.2],
            'classifier__max_depth': [3, 5]
        }
    }
    
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    
    if model_type in param_grids:
        grid_search = GridSearchCV(
            pipeline, 
            param_grids[model_type],
            cv=cv, 
            scoring='roc_auc',
            n_jobs=-1,
            verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        
        print(f"‚úÖ –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {model_type}: {grid_search.best_params_}")
        print(f"‚úÖ –õ—É—á—à–∏–π ROC-AUC: {grid_search.best_score_:.4f}")
        
        return grid_search.best_estimator_, grid_search.best_params_
    else:
        print(f"‚ö†Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è {model_type} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        pipeline.fit(X_train, y_train)
        return pipeline, {}

def evaluate_model(model, X_test, y_test, feature_names=None):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏"""
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'roc_auc': roc_auc_score(y_test, y_pred_proba),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1': f1_score(y_test, y_pred),
        'accuracy': (y_pred == y_test).mean()
    }
    
    # ROC curve
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {metrics["roc_auc"]:.3f})', linewidth=2)
    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.grid(True, alpha=0.3)
    
    # Feature importance (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
    plt.subplot(1, 3, 2)
    if hasattr(model.named_steps['classifier'], 'feature_importances_') and feature_names:
        importances = model.named_steps['classifier'].feature_importances_
        indices = np.argsort(importances)[::-1][:10]  # –¢–æ–ø-10 —Ñ–∏—á
        
        plt.barh(range(len(indices)), importances[indices], color='skyblue')
        plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
        plt.xlabel('Feature Importance')
        plt.title('Top 10 Feature Importances')
        plt.gca().invert_yaxis()
    
    # Distribution of predictions
    plt.subplot(1, 3, 3)
    plt.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, label='No Default', color='green')
    plt.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, label='Default', color='red')
    plt.xlabel('Predicted Probability')
    plt.ylabel('Frequency')
    plt.title('Distribution of Predictions')
    plt.legend()
    
    plt.tight_layout()
    roc_curve_path = 'reports/model_evaluation.png'
    plt.savefig(roc_curve_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # Classification report
    class_report = classification_report(y_test, y_pred, output_dict=True)
    
    return metrics, roc_curve_path, class_report

def train_experiment():
    """–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º EDA insights –∏ GridSearchCV"""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X, y = load_data_with_features()
    if X is None:
        return
    
    # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"üìä Training set: {X_train.shape}, Test set: {X_test.shape}")
    print(f"üìä Default rate in train: {y_train.mean():.3f}, test: {y_test.mean():.3f}")
    
    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å GridSearchCV
    experiments = [
        {
            'name': 'Logistic Regression - GridSearch',
            'model_type': 'logistic',
            'use_grid_search': True,
            'base_params': {'random_state': 42, 'max_iter': 1000}
        },
        {
            'name': 'Random Forest - GridSearch', 
            'model_type': 'random_forest',
            'use_grid_search': True,
            'base_params': {'random_state': 42}
        },
        {
            'name': 'Gradient Boosting - GridSearch',
            'model_type': 'gradient_boosting', 
            'use_grid_search': True,
            'base_params': {'random_state': 42}
        },
        {
            'name': 'Logistic Regression - Balanced',
            'model_type': 'logistic',
            'use_grid_search': False,
            'base_params': {'C': 0.1, 'class_weight': 'balanced', 'random_state': 42, 'max_iter': 1000}
        },
        {
            'name': 'Random Forest - Default',
            'model_type': 'random_forest',
            'use_grid_search': False, 
            'base_params': {'n_estimators': 100, 'random_state': 42}
        }
    ]
    
    best_score = 0
    best_model = None
    best_experiment = None
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow
    mlflow.set_tracking_uri("mlruns")
    mlflow.set_experiment("Credit Scoring Experiments")
    
    for exp in experiments:
        with mlflow.start_run(run_name=exp['name']):
            print(f"\nüéØ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {exp['name']}")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞
            pipeline = create_model_pipeline(exp['model_type'], **exp['base_params'])
            
            # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å –±–∞–∑–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            if exp['use_grid_search']:
                model, best_params = hyperparameter_tuning(
                    pipeline, exp['model_type'], X_train, y_train
                )
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                for param, value in best_params.items():
                    mlflow.log_param(param, value)
            else:
                model = pipeline
                model.fit(X_train, y_train)
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                for param, value in exp['base_params'].items():
                    mlflow.log_param(param, value)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
            mlflow.log_param('model_type', exp['model_type'])
            mlflow.log_param('use_grid_search', exp['use_grid_search'])
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–º–µ–Ω —Ñ–∏—á –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            try:
                feature_names = get_feature_names(model, X_train.columns)
            except:
                feature_names = None
            
            # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
            metrics, roc_curve_path, class_report = evaluate_model(
                model, X_test, y_test, feature_names
            )
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            for metric_name, metric_value in metrics.items():
                mlflow.log_metric(metric_name, metric_value)
                print(f"   {metric_name}: {metric_value:.4f}")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            mlflow.log_artifact(roc_curve_path)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ classification report
            with open('reports/classification_report.json', 'w') as f:
                json.dump(class_report, f, indent=2)
            mlflow.log_artifact('reports/classification_report.json')
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            mlflow.sklearn.log_model(model, "model")
            
            print(f"‚úÖ {exp['name']} - ROC AUC: {metrics['roc_auc']:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if metrics['roc_auc'] > best_score:
                best_score = metrics['roc_auc']
                best_model = model
                best_experiment = exp['name']
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if best_model is not None:
        mlflow.sklearn.save_model(best_model, "models/best_model")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        model_info = {
            'best_experiment': best_experiment,
            'best_score': best_score,
            'model_type': best_model.named_steps['classifier'].__class__.__name__,
            'feature_importance': None
        }
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å feature importance, —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        if hasattr(best_model.named_steps['classifier'], 'feature_importances_'):
            try:
                feature_names = get_feature_names(best_model, X_train.columns)
                importances = best_model.named_steps['classifier'].feature_importances_
                feature_importance = dict(zip(feature_names, importances))
                model_info['feature_importance'] = feature_importance
                
                # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                plt.figure(figsize=(10, 8))
                top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:15]
                features, importance_vals = zip(*top_features)
                
                plt.barh(range(len(features)), importance_vals, color='lightblue')
                plt.yticks(range(len(features)), features)
                plt.xlabel('Feature Importance')
                plt.title('Top 15 Feature Importances - Best Model')
                plt.gca().invert_yaxis()
                plt.tight_layout()
                plt.savefig('reports/best_model_feature_importance.png', dpi=300, bbox_inches='tight')
                plt.close()
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è feature importance: {e}")
        
        with open('reports/best_model_info.json', 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨")
        print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {best_experiment}")
        print(f"   ROC AUC: {best_score:.4f}")
        print(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {model_info['model_type']}")
        print(f"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: models/best_model")

if __name__ == "__main__":
    train_experiment()